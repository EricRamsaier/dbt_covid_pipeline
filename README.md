# dbt COVID Pipeline

A dbt-based ELT pipeline that models and documents COVID-19 metrics from Our World in Data (OWID).  
Built on Snowflake, this project provides a clean, tested, and documented set of staging, intermediate, fact, dimension, and aggregate models, plus automated snapshots and exposures.

---

## ğŸš€ Project Overview

- **Source**: `raw.owid.owid_covid_data` (CSV from OWID public GitHub)  
- **Staging**: `models/staging/owid/stg_owid_covid_data`  
- **Intermediate**: `models/intermediate/owid/int_owid_covid_data`  
- **Dimensions**:  
  - `dim_owid_continent`  
  - `dim_owid_iso_code` (with `location` & `continent`)  
- **Fact**: `fct_owid_covid`  
- **Aggregates**:  
  - `agg_owid_covid_day`  
  - `agg_owid_covid_month`  
  - `agg_owid_covid_year`  
- **Snapshots**:  
  - `snap_dim_owid_iso_code`  
- **Exposures**:  
  - `covid_dashboard`  
  - `covid_yearly_dashboard`  
- **Macros**: audit columns, dev filters, RLS grants, etc.

---

## ğŸ“‹ Prerequisites

- **dbt** â‰¥ 1.4 (tested on 2025.5.7)  
- **Snowflake** account & role with permissions to `CREATE`/`SELECT` in your target schemas  
- **Python** (for any pre-commit linting or custom scripts)  
- (Optional) **SQLFluff** for style checks  

---

## ğŸ› ï¸ Setup

1. **Clone the repo**  
   ```bash
   git clone git@github.com:yourorg/dbt_covid_pipeline.git
   cd dbt_covid_pipeline
   ```

2. **Install dependencies**  
   ```bash
   pip install dbt-core dbt-snowflake
   # (or run your projectâ€™s `requirements.txt` if you have one)
   ```

3. **Configure profiles**  
   Edit `~/.dbt/profiles.yml` to add a `dbt_covid_pipeline` profile that points at your Snowflake account.

4. **Initialize the project**  
   ```bash
   dbt deps
   dbt seed    # if you have any seeds
   ```

---

## âš¡ Usage

- **Compile & Build**  
  ```bash
  dbt clean
  dbt build
  ```
  This will run all models, snapshots, tests, and then package your documentation.

- **Run Tests Only**  
  ```bash
  dbt test
  ```

- **Parse & Compile Only**  
  ```bash
  dbt parse
  dbt compile
  ```

- **Source Freshness**  
  ```bash
  dbt source freshness
  ```

- **Docs Generation & Preview**  
  ```bash
  dbt docs generate
  dbt docs serve
  ```
  Browse to `http://localhost:8080` and look under **Models**, **Snapshots**, **Exposures** (and **Metrics**, if/when enabled).

---

## ğŸŒ² Folder Structure

```
â”œâ”€â”€ analysis/                   â† ad-hoc analyses (not deployed)
â”œâ”€â”€ macros/                     â† reusable macro functions
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/owid/           â† raw-to-staging transformations
â”‚   â”œâ”€â”€ intermediate/owid/      â† cleaning, deduplication
â”‚   â”œâ”€â”€ marts/owid/             â† dims, facts, aggregates
â”‚   â””â”€â”€ metrics/                 â† (optional) semantic metrics YAMLs
â”œâ”€â”€ snapshots/owid/             â† snapshot definitions
â”œâ”€â”€ tests/                      â† custom schema or data tests
â”œâ”€â”€ docs/                       â† supplemental Markdown docs
â”œâ”€â”€ dbt_project.yml             â† project configuration
â””â”€â”€ README.md                   â† this file
```

---

## ğŸ”’ Governance & Best Practices

- **Contracts**: all marts models use `on_schema_change='fail'` and `contract={'enforced':true}`  
- **Tags**: consistent use of `tags: ['stg','int','dim','fct','agg','owid']` for grouping  
- **YAML Tests**: `not_null`, `unique`, and `relationships` tests defined in `schema.yml` files  
- **Commenting**: `-- Created` / `-- Last Modified` headers in SQL files, plus clear purpose/notes blocks  
- **CI/CD**: recommended via GitHub Actions or similar to run `dbt build` on PRs and auto-publish docs  

---

## ğŸ“ˆ CI/CD & Deployment

1. **On Pull Request**  
   - `dbt deps && dbt parse && dbt compile && dbt test`  
   - Lint with SQLFluff (optional)  
2. **On Merge to `main`**  
   - `dbt build`  
   - `dbt docs generate` â†’ publish to S3 or GitHub Pages  
3. **Alerts**  
   - Configure Slack/email notifications on test or freshness failures.

---

## ğŸ¤ Contributing

1. Branch off of `main` for each feature/fix.  
2. Submit PR against `main` with changes to models, tests, docs, or readme.  
3. Ensure all checks pass (build, tests, lint).  

---

## ğŸ“ Support

- **Owner**: eramsaier@gmail.com  
- **Team**: analytics_engineering@yourcompany.com  

---

_Last updated: 2025-05-10_
